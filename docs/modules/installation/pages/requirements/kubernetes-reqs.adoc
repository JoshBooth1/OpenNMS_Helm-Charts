* Kubernetes version 1.20+.

* All components on a single `namespace` represent a single OpenNMS environment or customer deployment or a single tenant.
The name of the `namespace` will be used as:
  * Customer/deployment identifier.
  * The name of the deployed Helm application.
  * A prefix for the OpenNMS and Grafana databases in PostgreSQL.
  * A prefix for the index names in Elasticsearch when processing flows.
  * A prefix for the topics in Kafka (requires configuring the OpenNMS instance ID on Minions).
  * A prefix for the Consumer Group IDs in OpenNMS and Sentinel.
  * Part of the subdomain used by the Ingress Controller to expose WebUIs.
  It should not contain special characters and must follow FQDN restrictions.

* A single instance of OpenNMS Core (backend) for centralized monitoring running ALEC in standalone mode (if enabled).
** OpenNMS doesn't support distributed mode, meaning the `StatefulSet` cannot have more than one replica.

* A shared volume for the RRD files, mounted as read-write on the Core instance, and as read-only on the UI instances if applicable.

* A shared volume for the core configuration files, mounted as read-only on the UI instances if applicable.
** Allows for sharing configuration across all the OpenNMS instances (for example, `users.xml`, and `groups.xml`).

* Multiple instances of Grafana (frontend), using PostgreSQL as the backend, pointing to the OpenNMS UI service when available.
** When UI instances are not present, the OpenNMS Helm Chart data sources point to the OpenNMS Core service.

* `Secrets` to store the credentials, certificates, and truststores.

* `ConfigMaps` to store initialization scripts and standard configuration settings.

* An `Ingress` to control TLS termination and provide access to all the components (using Nginx).footnote:[You can manage certificates using Let's Encrypt via `cert-manager`, but we only require the name of a `ClusterIssuer`.]
** To integrate with Google Cloud DNS managed zones or Azure DNS, we need a wild-card entry for the chosen domain against the IP of the Ingress Controller.

*Optional requirements for additional implementation scenarios*

* Multiple instances of read-only OpenNMS UI (frontend).
** Must be stateless (unconfigurable).
** The `Deployment` must work with multiple replicas.
** Any configuration change goes to the core server.

* Multiple instances of Sentinel to handle Flows (requires Elasticsearch as an external dependency).
** When Sentinels are present, `Telemetryd` is disabled on the OpenNMS Core instance.

* Custom `StorageClass` for shared content (Google Filestore or Azure Files) to use `ReadWriteMany`
** Required only when having dedicated OpenNMS UI instances or when using the default RRD storage for time series data (not Cortex); otherwise, the default `StorageClass` is used (for example, for Google Cloud, it would be `standard` based on `kubernetes.io/gce-pd`.)
** Use the same `UID` and `GID` as the OpenNMS image with proper file modes.
** Due to how Google Filestore works, we need to specify `securityContext.fsGroup` (not required for Azure Files).
Check https://github.com/kubernetes-sigs/gcp-filestore-csi-driver/blob/master/docs/kubernetes/fsgroup.md[CSI driver FsGroup User Guide] for more information.
** Keep in mind that the minimum size of a Google Filestore instance is 1TB.
** Keep in mind that a new PVC will be in place if the environment gets recreated, meaning new Filestore instances.


IMPORTANT: Unless you build custom KAR images for OpenNMS, the latest available version of the ALEC and TSS Cortex KAR plugins (when enabled) will be downloaded directly from GitHub every time the OpenNMS Core container starts, as those binaries are not part of the current Docker image for OpenNMS.
To get KAR plugins from Docker to avoid contacting GitHub, set `alecImage` and/or `cortexTssImage` values as appropriate.
See https://github.com/opennms-forge/onms-k8s-poc/blob/main/kar-containers/README.md[kar-containers/README.md] for information on the Docker containers.